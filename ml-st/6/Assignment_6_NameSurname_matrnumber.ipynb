{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "757f90eee91c495528c1689ad48bdcbe",
     "grade": false,
     "grade_id": "cell-e9c51e57a4035f07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 6: Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e07eaa4a2726238f7e1aac09781d2ce",
     "grade": false,
     "grade_id": "cell-3575f02ddfb31d20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Copyright and Fair Use</h2>\n",
    "\n",
    "This material, no matter whether in printed or electronic form,\n",
    "may be used for personal and non-commercial educational use\n",
    "only. Any reproduction of this material, no matter whether as a\n",
    "whole or in parts, no matter whether in printed or in electronic\n",
    "form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30c0d6ec8a87f78abba68973a664a2f1",
     "grade": false,
     "grade_id": "cell-64e866008c7a245f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Automatic Testing Guidelines</h2>\n",
    "\n",
    "Automatic unittesting requires you to submit a notebook which contains strictly defined objects.\n",
    "Strictness of definition consists of unified shapes, dtypes, variable names and more.\n",
    "\n",
    "Within the notebook, we provide detailed instruction which you should follow in order to maximise your final grade.\n",
    "\n",
    "**Name your notebook properly**, follow the pattern in the template name:\n",
    "\n",
    "**Assignment_N_NameSurname_matrnumber**\n",
    "<ol>\n",
    "    <li>N - number of assignment</li>\n",
    "    <li>NameSurname - your full name where every part of the name starts with a capital letter, no spaces</li>\n",
    "    <li>matrnumber - you student number on ID card (with k, potentially with a leading zero)</li>\n",
    "</ol>\n",
    "\n",
    "Don't add any cells but use the ones provided by us. All cells have a unique ID so that the unit test can find it, so please do not add or remove any cell!\n",
    "\n",
    "Always make sure that implemented functions have the correct output and given variables contain the correct data type. In the descriptions for every function you can find information on what datatype an output should have and you should stick to that in order to minimize conflicts with the unittest. Don't import any other packages than listed in the cell with the \"imports\" tag.\n",
    "\n",
    "Questions are usually multiple choice (except the task description says otherwise) and can be answered by changing the given variables to either \"True\" or \"False\". \"None\" is counted as a wrong answer in any case!\n",
    "\n",
    "**Note:** Never use variables you defined in another cell in your functions directly; always pass them to the function as a parameter. In the unitest, they won't be available either. If you want to make sure that everything is executable for the unittest, try executing cells/functions individually (instead of running the whole notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b0f513ac14a39ad2ac1084cd0f1941c",
     "grade": false,
     "grade_id": "cell-bf4ae01117702651",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nothing to do here, just run the cell.\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RSEED = 44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "307e3a50d009bcc5ef5af8300bdb09fd",
     "grade": false,
     "grade_id": "cell-2a4d20b1fd7131e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 1: Implementation of Logistic Regression</h2>\n",
    "\n",
    "The goal of this exercise is to implement logistic regression from scratch using only numpy - do not import any new packages! \n",
    "\n",
    "Complete the following tasks:\n",
    "\n",
    "* **Code 1.1**:\n",
    "    * Implement a function `calc_loss(w, X, y)`, that takes a parameter vector `w`, a data matrix `X` and a binary label vector `y` and returns the binary cross-entropy loss: $$L_{BCE} = -\\sum_{n = 1}^{N} \\boldsymbol{y}^n \\ln \\left(\\sigma(\\boldsymbol{x}^n\\cdot w)\\right) + ({1 - \\boldsymbol{y}^n}) \\ln \\left(1 - \\sigma(\\boldsymbol{x}^n\\cdot w)\\right)$$\n",
    "* **Code 1.2**:\n",
    "    * Implement the formula for the gradient computed in the lecture.\n",
    "    * In particular you should implement a function `analytical_gradient(w, X, y)` that takes the same parameters as the function before but returns the gradient $\\frac{\\partial L}{\\partial\\mathbf{w}}$, where $L$ is the binary cross-entropy loss.\n",
    "* **Code 1.3**:\n",
    "    * Test whether the gradient calculated by `analytical_gradient(w, X, y)` is correct via Gradient Checking. To do so, implement the function `numerical_gradient(w, X, y, loss_fn, eps)` that takes the same parameters as `logistic_gradient` as well as a loss function and an additional optional parameter `eps` and computes: $$\\operatorname{numerical gradient} = \\frac{f(x + \\epsilon) - f(x - \\epsilon)}{2\\epsilon}$$\n",
    "* **Code 1.4**:\n",
    "    * Implement the function `generate_random(N, K)` that generates a random data matrix consisting of `N` data points with `K` features drawn from a standard normal distribution as well as corresponding random binary labels and a random weight vector, whose entries again stem from the standard normal distribution.  Hint: to generate the distributions use `np.random.normal` and `np.random.randint`.\n",
    "* **Question 1.5**:\n",
    "    * Answer some questions about your results from the previous exercies.\n",
    "* **Question 1.6**:\n",
    "    * Answer some questions about the missing bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "728f221b812248fea5a387a75da49050",
     "grade": false,
     "grade_id": "cell-832642d74c482061",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Code 1.1 (5 Points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87c79d1b8c8b68ddfd4c3c08537fad99",
     "grade": false,
     "grade_id": "cost",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_loss(w: np.ndarray, X: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"Function that computes the cross-entropy loss.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w : (K,) np.ndarray\n",
    "        Weight vector.\n",
    "    X : (N, K) np.ndarray\n",
    "        Data matrix.\n",
    "    y : (N,) np.ndarray\n",
    "        Label vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The cross-entropy loss.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df353e649feb197168d0fa7a7acaa545",
     "grade": true,
     "grade_id": "cost_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "N, K = 20, 5\n",
    "w_testing = np.linspace(-3, 1, K)\n",
    "X_testing = np.linspace(-2, 2, N*K).reshape(N, K)\n",
    "y_testing = np.zeros(N)\n",
    "y_testing[[2, 5, 7, 8, 9, 14]] = 1\n",
    "\n",
    "loss = calc_loss(w_testing, X_testing, y_testing)\n",
    "assert isinstance(loss, float), \"The resulting loss is not a float!\"\n",
    "assert np.isclose(loss, 41.6671, atol=1e-4), \"The loss is incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dabb2d2703953d4a067928e36fedf1b3",
     "grade": false,
     "grade_id": "cell-e4950b9b7c76e1af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Code 1.2 (5 Points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a8d8bb059c19958a4e827fcc72a97cf",
     "grade": false,
     "grade_id": "logistic_gradient",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analytical_gradient(w: np.ndarray, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Function that computes the logistic gradient via the cross-entropy loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : (K,) np.ndarray\n",
    "        Weight vector.\n",
    "    X : (N, K) np.ndarray\n",
    "        Data matrix.\n",
    "    y : (N,) np.ndarray\n",
    "        Label vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gradient : (K,) np.ndarray\n",
    "        Gradient vector.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52eff112b536542ba56b68fcf9924130",
     "grade": true,
     "grade_id": "logistic_gradient_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "analytical_grad = analytical_gradient(w_testing, X_testing, y_testing)\n",
    "assert isinstance(analytical_grad, np.ndarray), \"The gradient is not a np.ndarray!\"\n",
    "np.testing.assert_array_almost_equal(analytical_grad, np.array([-7.6822332 , -7.50445673, -7.32668026, -7.14890379, -6.97112732]), decimal=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba6b8ead1817b74fb6c3a945d606bc00",
     "grade": false,
     "grade_id": "cell-5236323b6e44e406",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Code 1.3 (14 Points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e3322139c7d03fc09f45e696bbecb72",
     "grade": false,
     "grade_id": "numerical_gradient",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(w: np.ndarray, X: np.ndarray, y: np.ndarray, loss_fn: callable, eps: float = 1e-7) -> np.ndarray:\n",
    "    \"\"\"Function that computes the numerical gradient for each epsilon in eps_list.\n",
    "    Hint: use the previously implemented cost function. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : (K,) np.ndarray\n",
    "        Weight vector.\n",
    "    X : (N, K) np.ndarray\n",
    "        Data matrix.\n",
    "    y : (N,) np.ndarray\n",
    "        Label vector.\n",
    "    loss_fn: callable\n",
    "        Loss function that calculates the cross-entropy loss.\n",
    "    eps : float = 1e-7\n",
    "        The epsilon value for the difference quotient calculation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gradient : (K,) np.ndarray\n",
    "        The numerical gradient approximating the analytical one.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93adb8a7b1b37fd9f2a6fcf66e2c389b",
     "grade": true,
     "grade_id": "numerical_gradient_test",
     "locked": true,
     "points": 14,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "numerical_grad = numerical_gradient(w_testing, X_testing, y_testing, lambda x, y, z: np.sum(np.tanh(x)))\n",
    "\n",
    "assert isinstance(numerical_grad, np.ndarray), \"The numerical gradient is not a np.ndarray!\"\n",
    "np.testing.assert_array_almost_equal(numerical_grad, np.array([0.00986604, 0.07065083, 0.41997434, 1., 0.41997434]), decimal=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c20e46c90454a8733a1a9fc1281045b4",
     "grade": false,
     "grade_id": "cell-af7aac53fe23ff26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Code 1.4 (10 Points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a4f630592cd6aa05cc53fe5336253aa",
     "grade": false,
     "grade_id": "generate_random",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_random(N: int, K: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Function that generates a random data matrix X and the random binary labels vectorsy as well as weights according to input specifications.\n",
    "    \n",
    "    Important: To generate the distributions use numpy's np.random.<...> functions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int,\n",
    "        Number of samples to generate.\n",
    "    K : int,\n",
    "        number of features of each sample.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple of np.ndarray\n",
    "        - X_random : (N, K) np.ndarray\n",
    "            The generated data matrix.\n",
    "        - y_random : (N,) np.ndarray\n",
    "            The generated binary labels vector.\n",
    "        - w_random : (K,) np.ndarray\n",
    "            The generated weights.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db04e597c2e2755a6d5600d567df159d",
     "grade": true,
     "grade_id": "generate_random_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "np.random.seed(RSEED)\n",
    "X_random, y_random, w_random = generate_random(5, 10)\n",
    "\n",
    "assert isinstance(X_random, np.ndarray) and isinstance(y_random, np.ndarray) and isinstance(w_random, np.ndarray), \"One of the results is not a np.ndarray!\"\n",
    "assert X_random.shape == (5, 10) and y_random.shape == (5,) and w_random.shape == (10,), \"At least one of the resulting arrays has a wrong shape!\"\n",
    "assert np.all(np.isin(y_random, [0, 1])), \"The generated labels vector has invalid values!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10bdb1bf5161ab60f995824a5e7cc5d0",
     "grade": false,
     "grade_id": "cell-69255c485ff05744",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nothing to do here, just run the cell.\n",
    "np.random.seed(RSEED)\n",
    "N = 5\n",
    "K = 10\n",
    "eps_list = [1e-1, 1e-4, 1e-11]\n",
    "X_random, y_random, w_random = generate_random(N, K)\n",
    "analytical_grad = analytical_gradient(w_random, X_random, y_random)\n",
    "numerical_grads = []\n",
    "for eps in eps_list:\n",
    "    numerical_grads.append(numerical_gradient(w_random, X_random, y_random, calc_loss, eps))\n",
    "    \n",
    "comparison_results=[np.allclose(analytical_grad, numerical_grad, atol=1e-7) for numerical_grad in numerical_grads]\n",
    "# Check outputs\n",
    "assert len(comparison_results) == len(eps_list), \"List with comparison results should be the same length as there are epsilon values.\"\n",
    "assert isinstance(comparison_results[0],bool), f\"Comparison results should be list of booleans.\"\n",
    "print(\"Your randomly generated data:\\n\")\n",
    "print(\"X =\",X_random,\"\\n\")\n",
    "print(\"y =\",y_random,\"\\n\")\n",
    "print(\"w = \",w_random,\"\\n\")\n",
    "print(\"Logistic gradient:\\n\",analytical_grad,\"\\n\")\n",
    "for i, res in enumerate(comparison_results):\n",
    "    eps_ = eps_list[i]\n",
    "    print(70*\"=\")\n",
    "    print(f\"Numerical gradient {i} with epsilon = {eps_}:\\n\", numerical_grads[i], \"\\n\")\n",
    "    print(\"Vectors within absolute tolerance of 10^-7: \", res, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2e842040b4234491bafd8d8439789a9",
     "grade": false,
     "grade_id": "cell-23c89fcfea0b61e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Question 1.5 (3 Points):</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "895638463ebce3fbdf8f69367157823a",
     "grade": false,
     "grade_id": "cell-3a9f504c3e972ee0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Q: Following the results from your comparison function, if at all, why do you think is the right choice of $\\epsilon$ crucial, for computing adequate approximations with the numerical gradient function?\n",
    "\n",
    "a1_) The choice does not matter, as long as epsilon is in (0,1) the function will give a satisfying solution, according to our tolerance criterion.<br>\n",
    "b1_) Very large values for $\\epsilon$, e.g. $\\epsilon=0.1$, lead to a divergence of the gradient as the denominator approaches zero. <br>\n",
    "c1_) Very small values for $\\epsilon$, e.g. $\\epsilon \\leq \\text{1e-11}$, can result in calculation errors, this could be due to issues with numerical precision, as both values in the numerator as well as in the denominator are very small.<br>\n",
    "\n",
    "To answer the question, assign `True` or `False` boolean values to variables in the next cell. For example, if you think that **a1_)** is correct, define a variable `a1_` and set it to `True`, the same applies to **b1_)** and the other options. A non-correctly answered question as well as no answer (i.e. answer “None”) yields 0 points for a specific question.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "935f072a8b10289d1dbf570c4a540f08",
     "grade": false,
     "grade_id": "question_1_5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a043f7228963c2b067623be4c018aa0",
     "grade": true,
     "grade_id": "a1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "assert a1_ is not None, \"Store True/False!\"\n",
    "assert a1_ in [True, False], \"Invalid Answer!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03b4cc2fb8f81a1a8fcf79ba6f7dbe36",
     "grade": true,
     "grade_id": "b1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "assert b1_ is not None, \"Store True/False!\"\n",
    "assert b1_ in [True, False], \"Invalid Answer!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f967ad69ee9e2ff208d8de7d18aaeee",
     "grade": true,
     "grade_id": "c1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "assert c1_ is not None, \"Store True/False!\"\n",
    "assert c1_ in [True, False], \"Invalid Answer!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a98119d601ab7a6b5fc22eab561c46b",
     "grade": false,
     "grade_id": "cell-ffc4de6e35f75db1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Question 1.6 (3 Points):</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0aac033c14985ba10c8c86c638623efc",
     "grade": false,
     "grade_id": "cell-52c798c478f85dd0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Q: Did you notice that we left out the bias term in our implementation of the logistic regression? (In the lecture Unit 5, Slide 45/47 shows the logistic unit with bias.) The bias $b$ represents the log-odds of the probability that the dependent variable takes on the value of 1 when all independent variables are set to zero.\n",
    "\n",
    "Why was it okay to not use a bias for our data? \n",
    "\n",
    "d1_) The bias term cancels out when calculating the derivative of the loss-function, because it does not depend on $\\mathbf{X}$.<br>\n",
    "e1_) The bias is only relevant when all independent variables are exactly 0, which never actually happens in real data.<br>\n",
    "f1_) The data follows a standard normal distribution, hence a bias of 0 leads to the correct predictions, based on the condition that the label distribution is symmetric. <br>\n",
    "\n",
    "To answer the question, assign `True` or `False` boolean values to variables in the next cell. For example, if you think that **d1_)** is correct, define a variable `d1_` and set it to `True`, the same applies to **e1_)** and the other options. A non-correctly answered question as well as no answer (i.e. answer “None”) yields 0 points for a specific question.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f16deaebad430f67aafde6b6cf9a242e",
     "grade": false,
     "grade_id": "question_1_6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20fcc4d3ac770c06f819fb90f0363763",
     "grade": true,
     "grade_id": "d1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "assert d1_ is not None, \"Store True/False!\"\n",
    "assert d1_ in [True, False], \"Invalid Answer!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35c7e9c5e959100ca29b498a5cdb2803",
     "grade": true,
     "grade_id": "e1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "assert e1_ is not None, \"Store True/False!\"\n",
    "assert e1_ in [True, False], \"Invalid Answer!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a497344cccf3746db56be65cdface71",
     "grade": true,
     "grade_id": "f1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "assert f1_ is not None, \"Store True/False!\"\n",
    "assert f1_ in [True, False], \"Invalid Answer!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc5fc76ae91d8805cbe20b9b91de736f",
     "grade": false,
     "grade_id": "cell-34dca17ba2a9a6a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 2: Logistic Regression on a real dataset</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "913894be6f2d48694bd86bc5860e4f30",
     "grade": false,
     "grade_id": "cell-e76ec96e5ae89f1f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Next we intend to apply logistic regression on a real data set. Firstly, in the following cell the data set `DataSet_LR_a.csv` is loaded and split into a training set and a test set ($50\\,\\%$ each). Also a constant feature (all ones) is added to X such that we can train an additional weight for the intercept which is the same for all datapoints. We visualize the dataset and then you should do the following:\n",
    "\n",
    "* **Code 2.1**:\n",
    "    * Implement a function `fit_Log_Reg_Model(...)` that uses Logistic Regression with Gradient Descent to train classifiers on the training set.\n",
    "    * Use randomly initialized weights, drawn from a uniform distribution between $-1$ and $1$ (have a look at `np.random.unifrom`), a default learning rate `eta` and a maximum number of iterations `max_iter`.\n",
    "    * Furthermore the algorithm should stop either if the difference between the loss of the last iteration step and the current loss is less than the `stopping_criterion` or the maximum of allowed iterations `max_iter` is reached (in this case print that the maximum iteration was reached).\n",
    "    * Also print the losses in $10000$ step intervals (also at step 0!). The function should return the fitted model weights.\n",
    "* **Code 2.2**:\n",
    "    * Furthermore, implement a function `predict_Log_Reg(w, X_test)` that returns the prediction for the given parameter vector $\\mathbf{w}$ and data matrix $\\mathbf{X_test}$.\n",
    "* **Question 2.3**:\n",
    "    * Answer some questions about your results.\n",
    "* **Code 2.4**:\n",
    "    * Implement the function `calc_metrics(prediction, targets, threshold)` that computes the Accuracy and Balanced Accuracy. To do so, you should  classify samples as class `1` if the Logistic Regression returns values $\\geq$`threshold` and `0` otherwise.\n",
    "* **Plot 2.5**:\n",
    "    * Plot the ROC curve of the classifiers on the test samples and compute and display the corresponding AUC. Hint: the functions `roc_curve` and `auc` from `sklearn.metrics` might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a15422cf52bbfd9e32e313867be79285",
     "grade": false,
     "grade_id": "cell-5ecb7a12380d75a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nothing to do here, just run the cell.\n",
    "# Read data, split into X(features) and y(labels)\n",
    "Z = np.genfromtxt('DataSet_LR_a.csv', delimiter=',',skip_header=1)\n",
    "X, y = Z[:,:-1], Z[:,-1]\n",
    "# Prepend ones for intercept\n",
    "X = np.hstack((np.ones((X.shape[0],1)),X))   \n",
    "\n",
    "# Plot data distribution\n",
    "color= ['red' if elem==1 else 'blue' for elem in y ]\n",
    "plt.scatter(X[:,-2], X[:,-1], c=color)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Complete dataset')\n",
    "\n",
    "# Split into test and training set\n",
    "X_train=X[:int(X.shape[0]/2)]\n",
    "X_test=X[int(X.shape[0]/2):]\n",
    "y_train=y[:int(len(y)/2)]\n",
    "y_test=y[int(len(y)/2):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a3afbe94ce1bde5288945966abd468f",
     "grade": false,
     "grade_id": "cell-d825b01ad2444f52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">Code 2.1 (20 Points)</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0c9f43758e6fa0c6ce418dc5a3deb37",
     "grade": false,
     "grade_id": "fit_Log_Reg_Model",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_Log_Reg_Model(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    loss_fn: callable,\n",
    "    grad_fn: callable, \n",
    "    eta: float = 1e-4,\n",
    "    stopping_criterion: float = 1e-5,\n",
    "    max_iter: float = 1e5\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Function that fits a logistic regression model to the given data. \n",
    "    Prints the loss every 10000 steps (also in the beginning at step 0).\n",
    "    Training stops if either the change of the current loss to the loss of the previous iteration is smaller than the stopping_criterion, or max_iter is reached.\n",
    "    \n",
    "    Important: Make sure to initialize the weights-vector from a random uniform distribution with np.random.uniform and with the correct dimensions according to X_train.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : (N, K) np.ndarray\n",
    "        Data matrix.\n",
    "    y_train : (N,) np.ndarray\n",
    "        Label vector.\n",
    "    loss_fn: callable\n",
    "        Loss function that calculates the binary cross-entropy loss.\n",
    "    grad_fn: callable\n",
    "        Function that calculates the analytical gradient.\n",
    "    eta : float\n",
    "        Learning rate for gradient descent.\n",
    "    stopping_criterion : float\n",
    "        Stopping criterion for the training process.\n",
    "    max_iter : int, optional\n",
    "        Max iteration at which training should stop, by default 100000.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weights : np.ndarray\n",
    "        The fitted weights.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e445a954ee970822272b8b77f0df6a6",
     "grade": true,
     "grade_id": "fit_Log_Reg_Model_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "np.random.seed(RSEED)\n",
    "\n",
    "w_learned = fit_Log_Reg_Model(X_train, y_train, calc_loss, analytical_gradient, eta=1e-4)\n",
    "print(\"The learnt weights are: w =\", w_learned, \"\\n\")\n",
    "assert isinstance(w_learned, np.ndarray), \"The resulting weights are not a np.ndarray!\"\n",
    "assert w_learned.shape == (X_train.shape[1],), \"The resulting weights have a wrong shape!\"\n",
    "np.testing.assert_almost_equal(w_learned, np.array([-3.26516093,  8.25477584, -1.49905643]), decimal=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ee2674e3302e9ad7fd2cbc8a02fac99",
     "grade": false,
     "grade_id": "cell-22aa04bdcc19b5dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">Code 2.2 (5 Points)</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "249c51726bf8c37f07aa054da90770e4",
     "grade": false,
     "grade_id": "predict_Log_Reg",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_Log_Reg(w: np.ndarray, X_test: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Function that calculates the prediction for one or more new and unseen samples, from the trained weights. \n",
    "    \n",
    "    Do not use any packages for this task except numpy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : (K,) np.ndarray\n",
    "        Vector of trained weights.\n",
    "    X_test : (N, K) np.ndarray\n",
    "        Samples to make predictions for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prediction : (N,) np.ndarray\n",
    "        Vector of calculated predictions, floats between 0 and 1\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41799b9ed6b02fcd8ed5a5355723da94",
     "grade": true,
     "grade_id": "predict_Log_Reg_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "pred = predict_Log_Reg(np.linspace(-5, 5, X_train.shape[1]), X_train[:5])\n",
    "assert isinstance(pred, np.ndarray), \"The predictions are not a np.ndarray!\"\n",
    "assert np.all(pred >=0) and np.all(pred <= 1), \"The predictions should be in the range [0, 1]!\"\n",
    "np.testing.assert_array_almost_equal(pred, np.array([0.05192384, 0.00680571, 0.466878  , 0.03788321, 0.26271971]), decimal=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a8c6c59d44c6b81b61b478cd12599eb3",
     "grade": false,
     "grade_id": "cell-9177a48cf2384757",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now we fit the logistic regression model to the training data with different learning rates and print the parameters for the test data. In case you want, you can try out different learning rates (just change the `eta_list`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the model on the training data with different learning rates.\n",
    "pred_train = []\n",
    "pred_test = []\n",
    "eta_list = [1e-7, 1e-4, 6e-1]\n",
    "\n",
    "for lr in eta_list:\n",
    "    np.random.seed(RSEED)\n",
    "    w_learned = fit_Log_Reg_Model(X_train, y_train, calc_loss, analytical_gradient, eta=lr)\n",
    "    pred_train.append(predict_Log_Reg(w_learned, X_train))\n",
    "    pred_test.append(predict_Log_Reg(w_learned, X_test))\n",
    "    print(\"The learnt weights are: w =\", w_learned, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a7f4055b52e35540c209abfd6f5db91",
     "grade": false,
     "grade_id": "cell-004b2b935fe43418",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's visualize your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9c195ddc1ecb83c0bbf981b02c0c949",
     "grade": false,
     "grade_id": "cell-bb0a5165d333d90d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nothing to do here, just run the cell.\n",
    "# Plot training and test dataset as well as the predictions for both\n",
    "for i, _ in enumerate(pred_train):\n",
    "    fig = plt.figure(figsize = (12,10))\n",
    "    fig.suptitle(f\"Results for eta = {eta_list[i]}\")\n",
    "    plt.subplot(2, 2, 1)\n",
    "    color= ['red' if elem>0.5 else 'blue' for elem in y_train ]\n",
    "    plt.scatter(X_train[:,-2], X_train[:,-1], c=color,label='the data')\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title('Training dataset')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    color= ['red' if elem>0.5 else 'blue' for elem in pred_train[i] ]\n",
    "    plt.scatter(X_train[:,-2], X_train[:,-1], c=color,label='the data')\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title('Training dataset - predictions')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    color= ['red' if elem>0.5 else 'blue' for elem in y_test ]\n",
    "    plt.scatter(X_test[:,-2], X_test[:,-1], c=color, label='the data')\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title('Test dataset')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    color= ['red' if elem>0.5 else 'blue' for elem in pred_test[i] ]\n",
    "    plt.scatter(X_test[:,-2], X_test[:,-1], c=color, label='the data')\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title('Test dataset - predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27f0c4816f51ed216b999c3150c68ef3",
     "grade": false,
     "grade_id": "cell-0ede66f0ce9a0213",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Question 2.3 (5 Points):</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02e2308a1ac3bd178180b7d2b2ef22ed",
     "grade": false,
     "grade_id": "cell-64e63900a4bd2120",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Q: Following your experiments with the the learning rate (eta), which of the following statements are correct?\n",
    "\n",
    "a2_) Learning rates > 0.5 lead to fast convergence (i.e. our default `stopping_criterion` was met) and reasonably good predictions on the test data.<br>\n",
    "b2_) For learning rates smaller than 1e-6 the algorithm does not stop with the default `stopping_criterion` before `max_iter` is reached. <br>\n",
    "c2_) When inspecting the plot for the default learning rate 1e-4, we see that also the outliers are classified correctly.<br>\n",
    "d2_) No matter what learning rate we choose from (0,1), if the algorithm converges, in the sense that it stops with the `stopping_criterion`, the resulting weights are always the same (within a small tolerance of $\\sim|0.1|$)<br>\n",
    "e2_) If we trained the classifier for a very long time, with the optimal learning rate, the resulting model would separate the training data perfectly (accuracy = 1), but it would perform much worse on the test data (overfitting).<br>\n",
    "\n",
    "To answer the question, assign `True` or `False` boolean values to variables in the next cell. For example, if you think that **a2_)** is correct, define a variable `a2_` and set it to `True`, the same applies to **b2_)** and the other options. A non-correctly answered question as well as no answer (i.e. answer “None”) yields 0 points for a specific question.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4980693f8af06d766a69e6495d19dfde",
     "grade": false,
     "grade_id": "question_2_3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1027f19677a6c7cfdffadd0d7ea97477",
     "grade": true,
     "grade_id": "a2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "assert a2_ is not None, \"Store True/False!\"\n",
    "assert a2_ in [True, False], \"Invalid Answer!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "554d284a00a281c315b0dc8ad5888218",
     "grade": true,
     "grade_id": "b2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "assert b2_ is not None, \"Store True/False!\"\n",
    "assert b2_ in [True, False], \"Invalid Answer!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b12dfc1e08dcce363cc65bfd7ecfd830",
     "grade": true,
     "grade_id": "c2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "assert c2_ is not None, \"Store True/False!\"\n",
    "assert c2_ in [True, False], \"Invalid Answer!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e7e958b9b830de3c90441287012f227",
     "grade": true,
     "grade_id": "d2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "assert d2_ is not None, \"Store True/False!\"\n",
    "assert d2_ in [True, False], \"Invalid Answer!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9eeecd58d90a6568f9928b8576564b1e",
     "grade": true,
     "grade_id": "e2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "assert e2_ is not None, \"Store True/False!\"\n",
    "assert e2_ in [True, False], \"Invalid Answer!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "507359c59b032a4ebc0ae2dcd87e73dc",
     "grade": false,
     "grade_id": "cell-d0a1ffe936a9cab6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">Code 2.4 (15 Points)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38d4a37cf02552b54ab341b280cd8d04",
     "grade": false,
     "grade_id": "calc_metrics",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_metrics(predictions: np.ndarray, targets: np.ndarray, threshold: float = 0.5) -> tuple[float, float]:\n",
    "    \"\"\"Function that calculates the accuracy as well as the balanced accuracy of the predictions vs. true labels.\n",
    "    \n",
    "    Again, only us numpy for this task.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : np.ndarray\n",
    "        The computed predictions between 0 and 1.\n",
    "    targets : np.ndarray\n",
    "        The true binary targets.\n",
    "    threshold : float, optional\n",
    "        Threshold for the label decision, by default 0.5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of float\n",
    "        acc : float \n",
    "            The calculated accuracy.\n",
    "        balanced_acc :  float\n",
    "            The calculated balanced accuracy.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "122a2680090dd2a900ea879cb0a98b53",
     "grade": true,
     "grade_id": "cacl_metrics_acc",
     "locked": true,
     "points": 7.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "acc, bacc = calc_metrics(np.linspace(0, 1, len(y_train)), y_train)\n",
    "assert isinstance(acc, float), \"The accuracy is not a float!\"\n",
    "assert np.isclose(acc, 0.5667, atol=1e-4), \"The accuracy is not correct!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fffc218c5debdab58e6433e65af939b",
     "grade": true,
     "grade_id": "cacl_metrics_bacc",
     "locked": true,
     "points": 7.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "assert isinstance(bacc, float), \"The balanced accuracy is not a float!\"\n",
    "assert np.isclose(bacc, 0.5679, atol=1e-4), \"The balanced accuracy is not correct!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "106af151c3865716469a97ec56f7786e",
     "grade": false,
     "grade_id": "cell-6dbce89f7bba8c38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nothing to do here, just run the cell.\n",
    "# Calculate metrics for both the train and test set\n",
    "np.random.seed(RSEED)\n",
    "w_learned = fit_Log_Reg_Model(X_train, y_train, calc_loss, analytical_gradient, eta=1e-4)\n",
    "pred_train = predict_Log_Reg(w_learned, X_train)\n",
    "pred_test = predict_Log_Reg(w_learned, X_test)\n",
    "\n",
    "result_train = calc_metrics(pred_train, y_train)\n",
    "result_test = calc_metrics(pred_test, y_test)\n",
    "print(\"\\nAccuracy on training data:\", result_train[-2])\n",
    "print(\"Balanced accuracy on training data:\", result_train[-1])\n",
    "print(\"Accuracy on test data:\", result_test[-2])\n",
    "print(\"Balanced accuracy on test data:\", result_test[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4e5517c4917e3986baaf8b973e161d6",
     "grade": false,
     "grade_id": "cell-92182543cbe1029f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">Plot 2.5 (15 Points)</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49f13a35fa3f8b358f90d72d91a25cdd",
     "grade": true,
     "grade_id": "roc_auc",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def roc_auc(y_test: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"Function that computes the AUC of the input predictions vs the true labels and plots the ROC curve as well as displays the AUC in the plot.\n",
    "    \n",
    "    You are only allowed to use the imported sklearn functions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_test : np.ndarray\n",
    "        True labels of test data.\n",
    "    y_pred : np.ndarray\n",
    "        Predictions of test labels.\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65510ca1bf3cc0c934d79a432921b65b",
     "grade": false,
     "grade_id": "cell-495af5bf7573b4c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Nothing to do here, just run the cell.\n",
    "roc_auc(y_test, pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
